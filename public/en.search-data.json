{"/docs/":{"data":{"":"","going-deep-with-sres#Going deep with SREs":"I had the opportunity to do some dedicated work on tutorials (or how-to guides).","learning-about-genai#Learning about GenAI":"The site is hosted by Netlify and built on Hugo with the Hextra theme. I’ve added shortcode to display my resume as PDF and made some CSS changes for style.\nI’ve curated these docs to capture different docs types, per Diataxis: conceptual, procedural, reference, and tutorial.\nAll docs were written in VS Code. I collaborated with stakeholders through GitHub using git version control. I took product screenshots with SnagIt. To avoid conflicts of interest, I’ve removed the original images and replaced them with something more whimsical. I recommend checking out my GitHub account if you’re curious about how I provide feedback to engineering and product SMEs, or how I might manage sweeping changes to information architecture.\nUntangling the AWS story\rtl;dr: You can collect AWS metrics and store them in Company’s platform by polling individual API endpoints for each AWS service, or by integrating an agent with an AWS service. Company wanted to position the integration as our primary, recommended method.\nThe tricky part is that product still supports the API polling method, so the docs needed to stay live. This resulted in a seemingly random intermixing of procedures across several docs without a clear recommendation about where to start. Docs were littered with callouts for a new integration, but didn’t describe value or who should opt in. The area lacked formal logos despite having a real urgency to use the integration.\nWith this context, I made some decisions:\nI created a single set up doc that functions as a “start here” for anyone wanting AWS data in the platform. The steps are for both first time users and existing users on the polling method. I updated the intro doc with clear messaging about what procedures exist and why you’d choose one over the other. I removed all of those bizarre, billboard-y style callouts about the integration across all docs. I preserved the set up instructions for the API polling method and grouped that doc with other alternative set-up procedures. (tl;dr Amazon is really complex and there were different procedures if you maintain EC2 instances, use their Kubernetes service, or are a GovCloud customer.) This project was ad hoc, ambiguous, and really fun. I learned a lot about AWS and got to do some IA restructuring, which is my favorite kind of docs work.\nLearning about GenAI\rThere’s a lot I could say about my one year journey supporting a feature for genAI-powered apps (hereby AI apps going forward, for brevity).\nFrom a professional POV, the project gave me real hands on experience working on a feature with industry ambiguity. Our user researcher described it well: GenAI is an area where practicioners learn as they do. I found myself learning alongside my SMEs while relying on academic research and tech news to fill in the blanks. I ventured lots of guesses about how embedding, tokenization, or various models might work, and those guesses were often inelegant at best. My SMEs were patient and ran the gamut of all possible tech roles: software architects, PMs, field folks, engineers, designers, you name it.\nAll of this, and I still needed to understand what our feature did and who our audience was.\nThe doc I’ve provided is an introduction doc that covers a mix of observability and AI concepts. Despite my reservations about AI, this project was a labor of love! I recommend exploring the entire area for all of the docs work I did in a few months.","untangling-the-aws-story#Untangling the AWS story":""},"title":"Portfolio notes~"},"/docs/ai-monitoring/":{"data":{"":"Monitoring for AI is our solution for application monitoring (APM) for AI. When you enable monitoring for AI, our agents can give you end-to-end visibility into performance, cost, and quality of supported models from vendors like OpenAI and BedRock. Explore how users interact with an AI assistant, dig into trace-level details about a model’s response to an AI event, and compare the performance of different models across app environments.","get-started-with-monitoring-for-ai#Get started with monitoring for AI":"Ready to get started? Make sure to confirm that you can instrument your AI library or framework. You may need to update the agent if you’ve already instrumented your app.\nWhen you’re ready, use our doc to manually install AI monitoring. This doc directs you to the relevant procedures for installing an APM agent, then walks you through configuring the agent for AI monitoring.","how-does-monitoring-for-ai-work#How does monitoring for AI work?":"To get started with monitoring for AI, you’ll instrument your AI-powered app with one of our APM agents. Once you’ve instrumented your app, you can enable monitoring for AI so the agent can capture LLM observability data.\nWhen your AI assistant receives a prompt and returns a response, the agent captures metric and event data generated from external LLMs and vector stores. Our agent can:\nParse information about completion, prompt, and response tokens Track requests and responses that pass through any of our supported models Correlate negative or positive feedback about a response from your end users You can access all this information and more from the New Relic platform, then create alerts and dashboards to help you effectively manage your AI data and improve performance.","improve-ai-performance-with-monitoring-for-ai#Improve AI performance with monitoring for AI":"AI monitoring can help you answer critical questions about AI app performance: are your end users waiting too long for a response? Is there a recent spike in token usage? Are there patterns of negative user feedback around certain topics? With AI monitoring, you can see data specific to the AI-layer:\nIdentify errors in specific prompt and response interactions from the response table. If you’re looking to make improvements to your AI models, learn how to analyze your model with trace-level data. If you’re using different models across app environments, you can compare the cost and performance of your apps before deploying. Are you concerned about data compliance? Learn how to create drop filters to drop sensitive data before you send it to New Relic. "},"title":"Intro to monitoring for AI"},"/docs/aws/":{"data":{"":"","integrate-cloudwatch-metric-streams-with-the-platform#Integrate CloudWatch Metric Streams with the platform":"","prerequisites#Prerequisites":"","whats-next#What\u0026rsquo;s next?":"You can collect metrics about your AWS services in a CloudWatch repository, then use AWS CloudWatch Metric Streams to direct a real-time stream of metrics to a destination of your choice, like to Company. Integrating AWS with Company lets you view your AWS data alongside your other observability entities, giving you a seamless monitoring experience as you troubleshoot and make improvements to your system environment.\nTo get started, you’ll follow three broad integration steps:\nDefining an HTTP endpoint set to either our US or EU datacenters Creating custom configurations for the kinds of AWS data you want to stream, based on your individual use case Adding your AWS account to your Company account Prerequisites\rBefore you get started, make sure to update the permissions to your AWS roles:\nAt minimum, create a ReadOnlyAccess policy and apply them to your AWS roles associated with your Company account. If you’re managing multiple AWS accounts, make sure you connect all of them to a single Company account. Integrate CloudWatch Metric Streams with the platform\rThese integration procedures use AWS console to complete set up. They assume you’ve created a Kinesis Data Firehose and metric stream in the past, but we’ll provide some external links to AWS docs to help you along the way.\nCreate and configure a Firehose delivery stream\rAWS manages your data streaming with Amazon Data Firehose, which lets you create a separate Kinesis Data Firehose to deliver your AWS metric data. After creating a new Firehose delivery stream, input the following configurations from AWS constole:\nDelivery stream configurations\rDestination parameters:\nSource: Direct PUT or other sources Data transformation: disabled Record format conversion: disabed Destination: Choose Company from the dropdown. Define these Company-specific configurations under Destination settings:\nHTTP endpoint URL - US Datacenter: https://aws-api.THISISFAKE.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.THISISFAKE.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only\nS3 bucket: select a bucket or create a new one to store metrics that failed to be sent.\nApply buffer conditions\nBuffer size: 1MB Buffer interval: 60 (seconds) For the Permissions IAM role, either create or update IAM role for Company.\nIf you manage multiple regions across different AWS accounts, create a separate Kinesis Data Firehose per each region and point them to Company.\nCreate and configure a new metric stream\rYou can think of a metric stream as a post office for your AWS metrics while the Firehose delivery system is the truck that delivers it to yourr destination. This step assigns your Firehose to a newly created metric stream, which ensures your AWS data is delivered to the platform.\nFrom AWS Console, find CloudWatch Service, choose Streams from the Metrics menu, then click Create a metric stream. Configure your metric stream based on your use case. For example, you may opt to use inclusion and exclusion filters to push some AWS service metrics to Platform, but not others. Select the Firehose delivery stream you created in the previous step. We recommend giving it a meaningful name, like our-company-name-metric-stream. Update the default output format to Open Telemetry 0.7. JSON is currently unsupported. Connect AWS and platform\rYou’ve finished the AWS portion of these procedures, so now it’s time to head to the platform. To add your account, go to PLATFORMWEBSITE \u003e TILE NAME HERE \u003e ANOTHER THING TO SELECT \u003e AWS. From the left menu, select Add an AWS account then select Use metric streams. The platform will walk you through the rest!\nValidate your data’s streaming\rWe recommend that you run a simple AWS query using our company’s SQL language. Find our query builder anchored at the bottom of the platform, then run this simple query:\nSELECT sum(aws.s3.5xxErrors) FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName\rWhat’s next?\rNow that you’re all set up, we recommend checking out some additional resources:\nCheck out how to create alerts, use tags, and manage your AWS data. Learn how to monitor your EC2 instances or your EKS cluster. Review the mechanics of querying with NRQL to get the most out of your data. "},"title":"Integrate with AWS"},"/docs/infra-monitoring/":{"data":{"":"","adjust-the-time-picker-to-find-when-a-change-first-occurred#Adjust the time picker to find when a change first occurred":"","explore-your-host-data-to-find-the-cause-of-an-outage#Explore your host data to find the cause of an outage":"","filter-by-app-name#Filter by app name":"","objectives#Objectives":"","whats-next-next#What\u0026rsquo;s next? [#next]":"Your infrastructure requires regular inspection and maintenance. Like a city planner designing a new road or bridge, decisions about what your system needs should come from careful observation about existing limitations. For cities, this looks like adding new lanes to major thoroughfares when traffic congestion increases in the morning or evening. For a tech stack, this looks like a series of resource decisions that affect the performance of your APIs, services, and apps.\nWith our infrastructure agent, you can regularly collect metrics and event data so you know if your infrastructure is scaling appropriately. What processes affect CPU and disk usage? How does an app behave just before a CPU spike? Are your hosts reacting to a database failure? How quickly you can answer these questions is the difference between an outright outage, a minor incident, or a smooth autoscale.\nObjectives\rThis doc helps synthesize a platform journey for troubleshooting app failure with host data. You will learn to:\nCreate effective filters to scope your infra data Choose the correct host(s) related to your failing app without navigating away from the infra UI Use the timepicker to find when a change occurred Explore your host data to find the cause of an outage\rIdentify failing hosts\rIf you’re uncertain about how to get started, we recommend initially scoping your hosts by alert severity. Using the summary page overview, you can see that there are three critical alert incidents happening in your system.\nYou can use the filter bar to view data only about those three critical alerts. In this case, your query would read alertSeverity = 'CRITICAL', which scopes your aggregated data from 83 hosts down to three.\nFilter by app name\rOnce you’ve identified a host related to the incident, you can click through to view data only about that host. In this scenario, we’ve chosen apache-svr01. Since we’re trying to solve an app-related issue, we’re starting at the service map on the host’s page. This map shows you what apps depend on your chosen host.\nFrom the apache-svr01 scoped view, we can see that two apps depend on this host. One is alerting.\nIn this case, the app Orders team is the failing app.\nReturn to the infrastructure summary page so you can update your query. We want to evaluate all hosts related to this app even if they’re not yet alerting. Viewing the problem host in context of its partner set improves your understanding of what’s causing the app failure. For instance, maybe the other hosts are approaching a threshold, or maybe you haven’t created an alert for those others hosts.\nAdjust the filter bar to show any hosts related to the Orders team app. Your query should now read apmApplicationNames = Orders team.\nThis filter widened the incident radius beyond your initial apache_svr01 host, but still kept your data scoped to a relevant set. From here, you can start digging deeper into what resource limitation is affecting performance.\nSince only a couple of these hosts are alerting, you can rule out a potential database issue, which would affect all of the hosts. Instead you might choose to dig deeper in the System, Network, Processes, Storage, or Docker containers tabs. The next doc in this series covers how to compare and correlate data behavior. Adjust the time picker to find when a change first occurred\rAdjusting the time picker lets you view how your data has changed over time. This action lets you track when a change first occurred. Let’s look at these metrics graphs toggled between 3 hours ago and 6 hours ago.\n\u003cimg\rtitle=\"Tiles and graphs set to a 3 hour time parameter\"\ralt=\"A screenshot displaying two metrics graphs and tiles from the past 6 hours.\"\rsrc=\"/images/infrastructure_screenshot-full_metrics-time1.webp\"\r/\u003e\r\u003cimg\rtitle=\"Tiles and graphs set to a 6 hour time parameter\"\ralt=\"A screenshot displaying two metrics graphs and tiles from the past 6 hours.\"\rsrc=\"/images/infrastructure_screenshot-full_metrics-time2.webp\"\r/\u003e\rYour time series at 6 hours doesn’t display an obvious increase in disk utilization. Toggled to a 3 hour parameter, you can see approximately when behavior started to change. Your metrics graphs give you a visual clue when a spike or drop occurs. If there’s been an unexpected increase in load, the Events tile will display either many or too few expected events. The Alerts tile displays the number of hosts that are currently alerting with critical or warning thresholds. A steady increase in alerts over time might indicate when a change escalated the incident behavior. The tiles and metrics graphs can help you triangulate the approximate time of an incident. This is especially helpful if the cause of an incident is due to a update from an external vendor, or a deployment from another team. If that is the case, your next step for digging deeper would change.\nWhat’s next? [#next]\rWe’ve introduced how to locate failing apps by evaluating your infrastructure data. By starting with the summary page, you can overview how your hosts are performing over time and identify which hosts are supporting failing apps.\nBut how do you use your infrastructure data to make a decision about resource allocation? The next doc covers how you can dig deeper on a more specific incident, like troubleshooting high CPU."},"title":"Troubleshoot apps with host data"},"/resume/":{"data":{"":"\rPrevious\rNext    \r/ [pdf]\rView the PDF file here."},"title":"resume"}}