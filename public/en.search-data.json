{"/docs/ai-monitoring/":{"data":{"":"Monitoring for AI is our solution for application monitoring (APM) for AI. When you enable monitoring for AI, our agents can give you end-to-end visibility into performance, cost, and quality of supported models from vendors like OpenAI and BedRock. Explore how users interact with an AI assistant, dig into trace-level details about a model’s response to an AI event, and compare the performance of different models across app environments.","get-started-with-monitoring-for-ai#Get started with monitoring for AI":"Ready to get started? Make sure to confirm that you can instrument your AI library or framework. You may need to update the agent if you’ve already instrumented your app.\nWhen you’re ready, use our doc to manually install AI monitoring. This doc directs you to the relevant procedures for installing an APM agent, then walks you through configuring the agent for AI monitoring.","how-does-monitoring-for-ai-work#How does monitoring for AI work?":"To get started with monitoring for AI, you’ll instrument your AI-powered app with one of our APM agents. Once you’ve instrumented your app, you can enable monitoring for AI so the agent can capture LLM observability data.\nWhen your AI assistant receives a prompt and returns a response, the agent captures metric and event data generated from external LLMs and vector stores. Our agent can:\nParse information about completion, prompt, and response tokens Track requests and responses that pass through any of our supported models Correlate negative or positive feedback about a response from your end users You can access all this information and more from the New Relic platform, then create alerts and dashboards to help you effectively manage your AI data and improve performance.","improve-ai-performance-with-monitoring-for-ai#Improve AI performance with monitoring for AI":"AI monitoring can help you answer critical questions about AI app performance: are your end users waiting too long for a response? Is there a recent spike in token usage? Are there patterns of negative user feedback around certain topics? With AI monitoring, you can see data specific to the AI-layer:\nIdentify errors in specific prompt and response interactions from the response table. If you’re looking to make improvements to your AI models, learn how to analyze your model with trace-level data. If you’re using different models across app environments, you can compare the cost and performance of your apps before deploying. Are you concerned about data compliance? Learn how to create drop filters to drop sensitive data before you send it to New Relic. "},"title":"Intro to monitoring for AI"},"/docs/aws/":{"data":{"":"","integrate-cloudwatch-metric-streams-with-the-platform#Integrate CloudWatch Metric Streams with the platform":"","prerequisites#Prerequisites":"","whats-next#What\u0026rsquo;s next?":"You can collect metrics about your AWS services in a CloudWatch repository, then use AWS CloudWatch Metric Streams to direct a real-time stream of metrics to a destination of your choice, like to Company. Integrating AWS with Company lets you view your AWS data alongside your other observability entities, giving you a seamless monitoring experience as you troubleshoot and make improvements to your system environment.\nTo get started, you’ll follow three broad integration steps:\nDefining an HTTP endpoint set to either our US or EU datacenters Creating custom configurations for the kinds of AWS data you want to stream, based on your individual use case Adding your AWS account to your Company account Prerequisites\rBefore you get started, make sure to update the permissions to your AWS roles:\nAt minimum, create a ReadOnlyAccess policy and apply them to your AWS roles associated with your Company account. If you’re managing multiple AWS accounts, make sure you connect all of them to a single Company account. Integrate CloudWatch Metric Streams with the platform\rThese integration procedures use AWS console to complete set up. They assume you’ve created a Kinesis Data Firehose and metric stream in the past, but we’ll provide some external links to AWS docs to help you along the way.\nCreate and configure a Firehose delivery stream\rAWS manages your data streaming with Amazon Data Firehose, which lets you create a separate Kinesis Data Firehose to deliver your AWS metric data. After creating a new Firehose delivery stream, input the following configurations from AWS constole:\nDelivery stream configurations\rDestination parameters:\nSource: Direct PUT or other sources Data transformation: disabled Record format conversion: disabed Destination: Choose Company from the dropdown. Define these Company-specific configurations under Destination settings:\nHTTP endpoint URL - US Datacenter: https://aws-api.THISISFAKE.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.THISISFAKE.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only\nS3 bucket: select a bucket or create a new one to store metrics that failed to be sent.\nApply buffer conditions\nBuffer size: 1MB Buffer interval: 60 (seconds) For the Permissions IAM role, either create or update IAM role for Company.\nIf you manage multiple regions across different AWS accounts, create a separate Kinesis Data Firehose per each region and point them to Company.\nCreate and configure a new metric stream\rYou can think of a metric stream as a post office for your AWS metrics while the Firehose delivery system is the truck that delivers it to yourr destination. This step assigns your Firehose to a newly created metric stream, which ensures your AWS data is delivered to the platform.\nFrom AWS Console, find CloudWatch Service, choose Streams from the Metrics menu, then click Create a metric stream. Configure your metric stream based on your use case. For example, you may opt to use inclusion and exclusion filters to push some AWS service metrics to Platform, but not others. Select the Firehose delivery stream you created in the previous step. We recommend giving it a meaningful name, like our-company-name-metric-stream. Update the default output format to Open Telemetry 0.7. JSON is currently unsupported. Add your AWS account to platform\rYou’ve finished the AWS portion of these procedures, so now it’s time to head to the platform. To add your account, go to PLATFORMWEBSITE \u003e TILE NAME HERE \u003e ANOTHER THING TO SELECT \u003e AWS. From the left menu, select Add an AWS account then select Use metric streams. The platform will walk you through the rest!\nValidate your data’s streaming\rWe recommend that you run a simple AWS query using our company’s SQL language. Find our query builder anchored at the bottom of the platform, then run this simple query:\nSELECT sum(aws.s3.5xxErrors) FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName\rWhat’s next?\rNow that you’re all set up, we recommend checking out some additional resources:\nCheck out how to create alerts, use tags, and manage your AWS data. Learn how to monitor your EC2 instances or your EKS cluster. Review the mechanics of querying with NRQL to get the most out of your data. "},"title":"Integrate with AWS"},"/docs/infra-monitoring/":{"data":{"":"Under construction!"},"title":"Track your infra resources"},"/docs/intro-portfolio/":{"data":{"":"","going-deep-with-sres#Going deep with SREs":"Even though Company offered some infrastructure solutions, they’re primarily known as being app monitoring-forward. As an associate writer, I didn’t fully appreciate what this meant or how a docs site might expose a product’s strengths and weaknesses.","learning-about-genai#Learning about genAI":"The site is hosted by Netlify and built on Hugo with the Hextra theme. I’ve added shortcode to display my resume as PDF and made some CSS changes for style.\nI’ve curated these docs to capture different docs types, per Diataxis: conceptual, procedural, reference, and tutorial.\nAll docs were written in VS Code. I collaborated with stakeholders through GitHub using git version control. I took product screenshots with SnagIt. To avoid conflicts of interest, I’ve removed the original images and replaced them with something more whimsical. I recommend checking out my GitHub account if you’re curious about how I provide feedback to engineering and product SMEs, or how I might manage sweeping changes to information architecture.\nUntangling the AWS story\rtl;dr: You can collect AWS metrics and store them in Company’s platform by polling individual API endpoints for each AWS service, or by integrating an agent with an AWS service. Company wanted to position the integration as our primary, recommended method.\nThe tricky part is that product still supports the API polling method, so the docs needed to stay live. This resulted in a seemingly random intermixing of procedures across several docs without a clear recommendation about where to start. Docs were littered with callouts for a new integration, but didn’t describe value or who should opt in. The area lacked formal logos despite having a real urgency to use the integration.\nWith this context, I made some decisions:\nI created a single set up doc that functions as a “start here” for anyone wanting AWS data in the platform. The steps are for both first time users and existing users on the polling method. I updated the intro doc with clear messaging about what procedures exist and why you’d choose one over the other. I removed all of those bizarre, billboard-y style callouts about the integration across all docs. I preserved the set up instructions for the API polling method and grouped that doc with other alternative set-up procedures. (tl;dr Amazon is really complex and there were different procedures if you maintain EC2 instances, use their Kubernetes service, or are a GovCloud customer.) This project was ad hoc, ambiguous, and really fun. I learned a lot about AWS and got to do some IA restructuring, which is my favorite kind of docs work.\nLearning about genAI\rThere’s a lot I could say about my one year journey supporting a feature for genAI-powered apps (hereby AI apps going forward, for brevity).\nFrom a professional POV, the project gave me real hands on experience working on a feature with industry ambiguity. Our user researcher described it well: GenAI is an area where practicioners learn as they do. I found myself learning alongside my SMEs while relying on academic research and tech news to fill in the blanks. I ventured lots of guesses about how embedding, tokenization, or various models might work, and those guesses were often inelegant at best. My SMEs were patient and ran the gamut of all possible tech roles: software architects, PMs, field folks, engineers, designers, you name it.\nAll of this, and I still needed to understand what our feature did and who our audience was.\nThe doc I’ve provided is an introduction doc that covers a mix of observability and AI concepts. Despite my reservations about AI, this project was a labor of love! I recommend exploring the entire area for all of the docs work I did in a few months.","untangling-the-aws-story#Untangling the AWS story":""},"title":"Portfolio notes~"},"/resume/":{"data":{"":"\rPrevious\rNext    \r/ [pdf]\rView the PDF file here."},"title":"resume"}}