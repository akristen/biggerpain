{"/docs/":{"data":{"":"","going-deep-with-sres#Going deep with SREs":"I created some how to guides in FY23 to help customers navigate the platform’s dense UI. Since observability implies a large amount of data coming in about your system, I wanted these docs to help SREs (or whomever) chunk the UI into its relevant parts, then follow the data to identify root cause. I kept the story relatively simple for a couple reasons:\nSince Company’s customer base included mostly app developers, I thought a simple story could help them “think like an SRE.” If an SRE used the doc but had ample experience, I suspected they could take this basic framework for working with host and app data and complexify it to fit their use cases. These how to guides gave me plenty of exposure to our field and support teams. While I chatted with product and engineering to get a sense of why they made certain usability decisions, it was important to me that I wrote a story about how our users are actually using the product, rather than how we expected them to behave.","learning-about-genai#Learning about GenAI":"The site is hosted by Netlify and built on Hugo with the Hextra theme. I’ve added shortcode to display my resume as PDF and made some CSS changes for style.\nI’ve curated these docs to capture different docs types, per Diátaxis: conceptual, procedural, reference, and tutorial. The docs are shortened and don’t reflect the full scope of the live projects.\nAll docs were written in VS Code. I collaborated with stakeholders through GitHub using git version control. I took product screenshots with SnagIt. To avoid any conflicts, I’ve removed the original images and replaced them with something more whimsical. As an additional note, I’ve removed all hyperlinks but kept the Markdown format to give you a sense of when/how I link to other docs. I recommend checking out my GitHub account if you’re curious about how I provide feedback to engineering and product SMEs, or how I might manage sweeping changes to information architecture.\nLearning about GenAI\rThere’s a lot I could say about my one year journey supporting a feature for GenAI-powered apps.\nFrom a professional POV, the project gave me real hands on experience working on a feature with industry ambiguity. Our user researcher described it well: GenAI is an area where practicioners learn as they do. I found myself learning alongside my SMEs while relying on academic research and tech news to fill in the blanks.\nI ventured lots of guesses about how embedding, tokenization, or various models might work, and those guesses proved to be inelegant at best. But my SMEs were patient and together we pushed out a lot of docs content about a completely new feature.\nI’ve included two docs from this feature release. The first is an introduction doc that covers a mix of observability and AI concepts. The second is an API reference for building a function within an agent API to retrieve certain kinds of data from a GenAI-powered app.","untangling-the-aws-story#Untangling the AWS story":"tl;dr: You can collect AWS metrics and store them in Company’s platform by polling individual API endpoints for each AWS service, or by integrating an agent with an AWS service. Company wanted to position the integration as our primary, recommended method.\nThe tricky part is that product still supports the API polling method, so the docs needed to stay live. This resulted in a seemingly random intermixing of procedures across several docs without a clear recommendation about where to start. Docs were littered with callouts for a new integration, but didn’t describe value or who should opt in. The area lacked formal logos despite having a real urgency to use the integration.\nWith this context, I made some decisions:\nI created a single set up doc that functions as a “start here” for anyone wanting AWS data in the platform. The steps are for both first time users and existing users on the polling method. I updated the intro doc with clear messaging about what procedures exist and why you’d choose one over the other. I removed all of those bizarre, billboard-y style callouts about the integration across all docs. I preserved the set up instructions for the API polling method and grouped that doc with other alternative set-up procedures. (tl;dr Amazon is really complex and there were different procedures if you maintain EC2 instances, use their Kubernetes service, or are a GovCloud customer.) This project was ad hoc, ambiguous, and really fun. I learned a lot about AWS and got to do some IA restructuring, which is my favorite kind of docs work."},"title":"Portfolio notes~"},"/docs/ai-monitoring/":{"data":{"":"Monitoring for AI is our solution for application monitoring (APM) for AI. When you enable monitoring for AI, our agents can give you end-to-end visibility into performance, cost, and quality of supported models from vendors like OpenAI and BedRock. Explore how users interact with an AI assistant, dig into trace-level details about a model’s response to an AI event, and compare the performance of different models across app environments.","get-started-with-monitoring-for-ai#Get started with monitoring for AI":"Ready to get started? Make sure to confirm that you can instrument your AI library or framework. You may need to update the agent if you’ve already instrumented your app.\nWhen you’re ready, use our doc to manually install AI monitoring. This doc directs you to the relevant procedures for installing an APM agent, then walks you through configuring the agent for AI monitoring.","how-does-monitoring-for-ai-work#How does monitoring for AI work?":"To get started with monitoring for AI, you’ll instrument your AI-powered app with one of our APM agents. Once you’ve instrumented your app, you can enable monitoring for AI so the agent can capture LLM observability data.\nWhen your AI assistant receives a prompt and returns a response, the agent captures metric and event data generated from external LLMs and vector stores. Our agent can:\nParse information about completion, prompt, and response tokens Track requests and responses that pass through any of our supported models Correlate negative or positive feedback about a response from your end users You can access all this information and more from the New Relic platform, then create alerts and dashboards to help you effectively manage your AI data and improve performance.","improve-ai-performance-with-monitoring-for-ai#Improve AI performance with monitoring for AI":"AI monitoring can help you answer critical questions about AI app performance: are your end users waiting too long for a response? Is there a recent spike in token usage? Are there patterns of negative user feedback around certain topics? With AI monitoring, you can see data specific to the AI-layer:\nIdentify errors in specific prompt and response interactions from the response table. If you’re looking to make improvements to your AI models, learn how to analyze your model with trace-level data. If you’re using different models across app environments, you can compare the cost and performance of your apps before deploying. Are you concerned about data compliance? Learn how to create drop filters to drop sensitive data before you send it to New Relic. "},"title":"Intro to monitoring for GenAI"},"/docs/api/":{"data":{"":"","description#Description":"","example-obtain-trace-id-and-record-feedback#Example: Obtain trace ID and record feedback":"Syntax\rfakefunction.record_llm_feedback_event(trace_id, rating, category=None, message=None, metadata=None)\rRecords custom feedback events for GenAI LLMs.\nRequirements\rEnsure you have your API key and you’re using agent version 9.8.0 or higher.\nDescription\rThe agent API can record a feedback event LlmFeedbackMessage that describes whether an end user found an LLM response helpful. You need to create a function that correlates the feedback itself with the relevant LLM message. The agent API, however, records this data in different places since both events occur in two transactions. This means your function needs to:\nCapture the trace ID within the endpoint that generates the LLM message (fakefunction.agent.current_trace_id()). The trace ID acts as the connective tissue between feedback and LLM rresponse. Pass that trace ID to the endpoint that records the feedback (fakefunction.agent.record_llm_feedback_event()) Parameters\rParameter Description trace_id Required. ID of the trace where the chat completion(s) related to the feedback occurred. This ID can be obtained via a call to current_trace_id. rating Required. Rating provided by an end user (ex: “Good/Bad”, “1-10”). category Optional. Category of the feedback provided by the end user (ex: “informative”, “inaccurate”). message Optional. Freeform text feedback from an end user. metadata Optional. Set of key-value pairs to store any other desired data to submit with the feedback event. Return values\rNone.\nExample: Obtain trace ID and record feedback\rimport fake.agent def get_message(request): trace_id = fakefunction.agent.current_trace_id() def post_feedback(request): fakefunction.agent.record_llm_feedback_event(trace_id=request.trace_id, rating=request.rating, metadata= {\"my_key\": \"my_val\"})","parameters#Parameters":"","requirements#Requirements":"","return-values#Return values":"","syntax#Syntax":""},"title":"API reference `record_llm_feedback_event`"},"/docs/aws/":{"data":{"":"","integrate-cloudwatch-metric-streams-with-the-platform#Integrate CloudWatch Metric Streams with the platform":"","prerequisites#Prerequisites":"","whats-next#What\u0026rsquo;s next?":"You can collect metrics about your AWS services in a CloudWatch repository, then use AWS CloudWatch Metric Streams to direct a real-time stream of metrics to a destination of your choice, like to Company. Integrating AWS with Company lets you view your AWS data alongside your other observability entities, giving you a seamless monitoring experience as you troubleshoot and make improvements to your system environment.\nTo get started, you’ll follow three broad integration steps:\nDefining an HTTP endpoint set to either our US or EU datacenters Creating custom configurations for the kinds of AWS data you want to stream, based on your individual use case Adding your AWS account to your Company account Prerequisites\rBefore you get started, make sure to update the permissions to your AWS roles:\nAt minimum, create a ReadOnlyAccess policy and apply them to your AWS roles associated with your Company account. If you’re managing multiple AWS accounts, make sure you connect all of them to a single Company account. Integrate CloudWatch Metric Streams with the platform\rThese integration procedures use AWS console to complete set up. They assume you’ve created a Kinesis Data Firehose and metric stream in the past, but we’ll provide some external links to AWS docs to help you along the way.\nCreate and configure a Firehose delivery stream\rAWS manages your data streaming with Amazon Data Firehose, which lets you create a separate Kinesis Data Firehose to deliver your AWS metric data. After creating a new Firehose delivery stream, input the following configurations from AWS constole:\nDelivery stream configurations\rDestination parameters:\nSource: Direct PUT or other sources Data transformation: disabled Record format conversion: disabed Destination: Choose Company from the dropdown. Define these Company-specific configurations under Destination settings:\nHTTP endpoint URL - US Datacenter: https://aws-api.THISISFAKE.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.THISISFAKE.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only\nS3 bucket: select a bucket or create a new one to store metrics that failed to be sent.\nApply buffer conditions\nBuffer size: 1MB Buffer interval: 60 (seconds) For the Permissions IAM role, either create or update IAM role for Company.\nIf you manage multiple regions across different AWS accounts, create a separate Kinesis Data Firehose per each region and point them to Company.\nCreate and configure a new metric stream\rYou can think of a metric stream as a post office for your AWS metrics while the Firehose delivery system is the truck that delivers it to yourr destination. This step assigns your Firehose to a newly created metric stream, which ensures your AWS data is delivered to the platform.\nFrom AWS Console, find CloudWatch Service, choose Streams from the Metrics menu, then click Create a metric stream. Configure your metric stream based on your use case. For example, you may opt to use inclusion and exclusion filters to push some AWS service metrics to Platform, but not others. Select the Firehose delivery stream you created in the previous step. We recommend giving it a meaningful name, like our-company-name-metric-stream. Update the default output format to Open Telemetry 0.7. JSON is currently unsupported. Connect AWS and platform\rYou’ve finished the AWS portion of these procedures, so now it’s time to head to the platform. To add your account, go to PLATFORMWEBSITE \u003e TILE NAME HERE \u003e ANOTHER THING TO SELECT \u003e AWS. From the left menu, select Add an AWS account then select Use metric streams. The platform will walk you through the rest!\nValidate your data’s streaming\rWe recommend that you run a simple AWS query using our company’s SQL language. Find our query builder anchored at the bottom of the platform, then run this simple query:\nSELECT sum(aws.s3.5xxErrors) FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName\rWhat’s next?\rNow that you’re all set up, we recommend checking out some additional resources:\nCheck out how to create alerts, use tags, and manage your AWS data. Learn how to monitor your EC2 instances or your EKS cluster. Review the mechanics of querying with NRQL to get the most out of your data. "},"title":"Procedure to integrate AWS"},"/docs/infra-monitoring/":{"data":{"":"","make-a-resource-decision-about-an-outage#Make a resource decision about an outage":"","objectives#Objectives":"","whats-next#What\u0026rsquo;s next?":"Your infrastructure requires regular inspection and maintenance. Like a city planner designing a new road or bridge, decisions about what your system needs should come from careful observation about existing limitations. For a tech stack, this looks like a series of resource decisions that affect the performance of your APIs, services, and apps.\nWith our agent, you can collect metrics and event data to help you make resource decisions about your system infrastructure. This doc synthesizes a platform journey for troubleshooting app failure with host data.\nObjectives\rYou will learn to:\nScope your data to help you find root cause Make data-driven resource decisions using your host data The tutorial assumes you’ve already created your first alert.\nMake a resource decision about an outage\rStart with your alerts\rWhen you’re first building out your maintenace workflows, we recommend starting with Summary page overview until you find a better starting point for your unique use case. To start out, we’re going to head on over to PLATFORMWEBSITE \u003e TILE NAME HERE \u003e ANOTHER THING TO SELECT \u003e Infrastructure to view the summary page. This page shows you a lot of aggregated data about your system, but we can break this down into something more manageable.\nWe’ve divided your summary page into three sections:\nA row of four widgets that identify the particulars of your system, like the number of apps serviced by your hosts and how many alerts are notifying you about incidents in your system. A middle section with time series graphs showing you a high level overview of your CPU, memory, and disk resources. A summary table that breaks down these metrics by individual hosts. You might be familiar with our filter bar anchored to the top of all our capability pages. You can use this filter bar to scope the summary page to data about alerting entities. We’ve added this query to the filter bar: alertSeverity = 'CRITICAL'\nFind the failing app\rNotice how this query scoped the entire page from hundreds of hosts to three.\nThe CPU (%) time series indicates that:\napache-svr01 spiked from 15% CPU usage to just over 60% proxy-west-2 mimicked the behavior host-tower-chicago flatlined This isn’t enough information to declare our root cause, but it does limit the range of possibilities for our incident. There are any number of reasons for high CPU, but it’s likely the issue has something to do with either:\nThe app is running a redundant process that’s causing CPU to spike, so the app-owning team needs to optimize some code. More end users are accessing a certain component and adding stress to our system, so we need to provision more resources to meet that load. If we click the applications widget, our summary page opens a relationship modal that shows different apps serviced by various hosts. Note how the Orders team service is both alerting a critical incident and has a rrelationship to apache-svr01 (but not the other two).\nWe can exit the modal and return to the summary page so we can explore the overview page for apache-svr-01.\nDetermine root cause\rSince we’ve identified our fussy app, we can start testing our hypotheses from earlier. At this stage, the goal is to identify any patterns that help us answer the question: is app failure related to inefficient code or stress on the system?\nThe host overview page shows familiar widgets like CPU, memory, and disk usage, but also includes data about network, processes, load, and storage. To the right of the page is a sidebar that shows information about latest events limited to 30 minutes ago, any other related entities, and open issues your team’s created. This isn’t helpful to us right now, but we suspect you’ll encounter situations in the future where it can definitely help you.\nSince we’re disambiguating whether root cause is about app code or resources, we’ll take a quick glance at our Network and Processes widgets. We can see that:\nOur load average time series shows regular intervals for the last hour, but nothing that parallels our CPU % pattern. Network traffic shows regular interviews, but no parallels either. Our running processes show about a dozen different kinds of processes, but there’s a ruby process using 77.34% of your CPU. If load stress affected our CPU %, we’d expect to see a steep spike at the same time our CPU spiked, then a potential flatline when the app failed in the network and load graphs. Since we definitely don’t see that, we have pretty high confidence that the critical incident has to do with some kind of app process, not a resource one.\nSince we don’t need to provision any resources to our host, we can notify the app-owning team that some Ruby process is disrupting their Orders team service.\nℹ️\rIt’s best practice to check your Latest events sidebar to ensure someone hasn’t made direct changes to the machine. Look for any change in the machine’s config file or whether someone has entered the machine to make changes directly.\rWhat’s next?\rSo far we’ve covered how to use infrastructure data to troubleshoot a resource-related incident. We’ve covered how to scope down from thousands of hosts to a set of hosts, then correlated data to make a decision. The next doc shows you how to create custom dashboards using infrastructure metrics."},"title":"Tutorial to resolve app outage with host data"},"/resume/":{"data":{"":"\rPrevious\rNext    \r/ [pdf]\rView the PDF file here."},"title":"resume"}}